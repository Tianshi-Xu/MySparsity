2024-01-30 16:43:49,682 - train - INFO - aa: rand-m9-mstd0.5-inc1
amp: true
apex_amp: false
aq_asym: false
aq_bitw: null
aq_enable: false
aq_mode: lsq
aq_neg: null
aq_pos: null
as_enable: false
as_mode: Identity
as_patterns: '1:1'
aug_splits: 0
batch_size: 256
bn_eps: null
bn_momentum: null
bn_tf: false
channels_last: false
checkpoint_hist: 5
clip_grad: null
clip_mode: norm
color_jitter: 0.4
cooldown_epochs: 10
crop_pct: 1.0
cutmix: 1.0
cutmix_minmax: null
data_dir: /home/xts/code/dataset/cifar10
dataset: torch/cifar10
decay_epochs: 30
decay_rate: 0.1
dist_bn: ''
distributed: false
down_block_type: default
drop: 0.0
drop_block: null
drop_connect: null
drop_path: null
epoch_repeats: 0.0
epochs: 400
eval_metric: top1
experiment: ''
finetune: true
gp: null
hflip: 0.5
img_size: 32
initial_checkpoint: /home/xts/code/neujeans/MySparsity/Ternary-ViT/output/train/20240130-151816-pretrain_cifar_cir_nas_mobilenetv2-32/model_best.pth.tar
input_size: null
interpolation: bicubic
jsd: false
kd_alpha: 4
kd_type: last
lasso_alpha: 1.0e-05
local_rank: 0
log_interval: 50
log_wandb: false
lr: 0.00055
lr_cycle_limit: 1
lr_cycle_mul: 1.0
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
mean:
- 0.4914
- 0.4822
- 0.4465
min_lr: 1.0e-05
mixup: 0.8
mixup_mode: batch
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_switch_prob: 0.5
model: cifar_cir_nas_mobilenetv2
model_ema: false
model_ema_decay: 0.9998
model_ema_force_cpu: false
momentum: 0.9
native_amp: false
no_aug: false
no_bn: false
no_prefetcher: false
no_resume_opt: false
num_classes: 10
opt: adamw
opt_betas: null
opt_eps: null
output: ''
patience_epochs: 10
pin_mem: false
post_res_bn: false
pretrain: false
pretrained: false
qmodules: []
quant_teacher: false
ratio:
- 0.75
- 1.3333333333333333
recount: 1
recovery_interval: 0
remode: pixel
replace_ln_by_bn: false
replace_relu: false
reprob: 0.25
resplit: false
resq_asym: false
resq_bitw: null
resq_enable: false
resq_mode: lsq
resq_modules: []
resq_neg: null
resq_pos: null
resume: ''
save_images: false
scale:
- 0.8
- 1.0
sched: cosine
seed: 3407
smoothing: 0.1
split_bn: false
start_epoch: null
std:
- 0.247
- 0.2435
- 0.2616
sync_bn: false
teacher: pretrain_cifar_cir_nas_mobilenetv2
teacher_checkpoint: /home/xts/code/neujeans/MySparsity/Ternary-ViT/output/train/20240130-151816-pretrain_cifar_cir_nas_mobilenetv2-32/model_best.pth.tar
torchscript: false
train_interpolation: random
train_split: train
tta: 0
use_bn: true
use_distill_head: false
use_dual_skip: false
use_kd: true
use_layer_scale: false
use_multi_epochs_loader: false
use_relu: false
use_skip: false
use_token_kd: false
val_split: validation
validation_batch_size_multiplier: 1
vflip: 0.0
warmup_epochs: 0
warmup_lr: 1.0e-05
weight_decay: 0.06
workers: 32
wq_asym: false
wq_bitw: null
wq_enable: false
wq_mode: LSQ
wq_neg: null
wq_per_channel: false
wq_pos: null
ws_enable: false
ws_mode: Identity
ws_patterns: '1:1'

2024-01-30 16:43:49,683 - train - INFO - Training with a single process on 1 GPUs.
2024-01-30 16:43:49,837 - train - INFO - Model CirNasMobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): CirNasInvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=10, bias=True)
  )
)
pretrain: False
fine-tune: False
2024-01-30 16:43:51,069 - train - INFO - Model cifar_cir_nas_mobilenetv2 created, param count:2236824
2024-01-30 16:43:51,081 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-01-30 16:43:51,081 - train - INFO - Scheduled epochs: 410
2024-01-30 16:43:52,556 - train - INFO - Trainer transform: Compose(
    RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear bicubic)
    RandomHorizontalFlip(p=0.5)
    RandAugment(n=2, ops=
	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
    <timm.data.transforms.ToNumpy object at 0x7f786765fbb0>
)
2024-01-30 16:43:52,556 - train - INFO - Validate transform: Compose(
    Resize(size=32, interpolation=bicubic, max_size=None, antialias=warn)
    CenterCrop(size=(32, 32))
    <timm.data.transforms.ToNumpy object at 0x7f786765f6d0>
)
2024-01-30 16:43:52,556 - train - INFO - Train loss CrossEntropyLoss()
2024-01-30 16:43:52,556 - train - INFO - Verifying teacher model
2024-01-30 16:43:55,592 - train - INFO - Test: [   0/39]  Time: 3.034 (3.034s) Loss:  0.3721 (0.3721)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:43:56,348 - train - INFO - Test: [  39/39]  Time: 0.415 (3.791s) Loss:  0.4368 (0.3827)  Acc@1: 93.7500 (94.7500)  Acc@5: 100.0000 (99.8400)
2024-01-30 16:43:56,349 - train - INFO - Verifying initial model
2024-01-30 16:43:56,610 - train - INFO - Test: [   0/39]  Time: 0.260 (0.260s) Loss:  3.0488 (3.0488)  Acc@1:  8.9844 ( 8.9844)  Acc@5: 48.8281 (48.8281)
2024-01-30 16:44:02,319 - train - INFO - Test: [  39/39]  Time: 0.135 (5.968s) Loss:  2.7207 (2.9991)  Acc@1: 18.7500 (10.0000)  Acc@5: 50.0000 (50.0000)
2024-01-30 16:44:07,166 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  2.130087 (2.1301)  Time: 4.844s (4.844s),   52.85/s  (4.844s,   52.85/s)  LR: 5.500e-04  Data: 2.305 (2.305)
2024-01-30 16:44:34,944 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  1.759459 (1.7603)  Time: 0.535s (32.620s),  478.12/s  (0.640s,  400.24/s)  LR: 5.500e-04  Data: 0.006 (0.053)
2024-01-30 16:45:03,184 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  1.532155 (1.6813)  Time: 0.553s (60.860s),  462.54/s  (0.603s,  424.85/s)  LR: 5.500e-04  Data: 0.008 (0.030)
2024-01-30 16:45:31,270 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  1.749825 (1.6505)  Time: 0.503s (88.944s),  508.86/s  (0.589s,  434.61/s)  LR: 5.500e-04  Data: 0.005 (0.023)
2024-01-30 16:45:54,637 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  1.388477 (1.6239)  Time: 0.507s (112.310s),  504.79/s  (0.576s,  444.48/s)  LR: 5.500e-04  Data: 0.000 (0.019)
2024-01-30 16:45:54,642 - train - INFO - alphas:tensor([0.3580, 0.3236, 0.3185], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,643 - train - INFO - alphas:tensor([0.3599, 0.3199, 0.3202], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,643 - train - INFO - alphas:tensor([0.3627, 0.3211, 0.3162], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,644 - train - INFO - alphas:tensor([0.3579, 0.3230, 0.3191], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,645 - train - INFO - alphas:tensor([0.3617, 0.3183, 0.3200], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,646 - train - INFO - alphas:tensor([0.2164, 0.2006, 0.1921, 0.1941, 0.1967], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,646 - train - INFO - alphas:tensor([0.3625, 0.3199, 0.3176], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,647 - train - INFO - alphas:tensor([0.3596, 0.3220, 0.3184], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,648 - train - INFO - alphas:tensor([0.3607, 0.3205, 0.3187], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,649 - train - INFO - alphas:tensor([0.3591, 0.3225, 0.3184], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,649 - train - INFO - alphas:tensor([0.2195, 0.2028, 0.1914, 0.1920, 0.1943], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,650 - train - INFO - alphas:tensor([0.2178, 0.2029, 0.1929, 0.1927, 0.1938], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,651 - train - INFO - alphas:tensor([0.2193, 0.2023, 0.1925, 0.1916, 0.1942], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,652 - train - INFO - alphas:tensor([0.2198, 0.2009, 0.1904, 0.1923, 0.1966], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,653 - train - INFO - alphas:tensor([0.2207, 0.2029, 0.1900, 0.1915, 0.1950], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,653 - train - INFO - alphas:tensor([0.2195, 0.2042, 0.1895, 0.1919, 0.1949], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,654 - train - INFO - alphas:tensor([0.2204, 0.2010, 0.1920, 0.1923, 0.1945], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,655 - train - INFO - alphas:tensor([0.2175, 0.2031, 0.1908, 0.1920, 0.1965], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,656 - train - INFO - alphas:tensor([0.2228, 0.1991, 0.1916, 0.1922, 0.1942], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,657 - train - INFO - alphas:tensor([0.2217, 0.2008, 0.1913, 0.1921, 0.1941], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,658 - train - INFO - alphas:tensor([0.2230, 0.2007, 0.1908, 0.1920, 0.1935], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,659 - train - INFO - alphas:tensor([0.2201, 0.2006, 0.1918, 0.1933, 0.1943], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,659 - train - INFO - alphas:tensor([0.2226, 0.2005, 0.1901, 0.1917, 0.1951], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,660 - train - INFO - alphas:tensor([0.2220, 0.2015, 0.1907, 0.1913, 0.1945], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,661 - train - INFO - alphas:tensor([0.2226, 0.2015, 0.1907, 0.1916, 0.1936], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,662 - train - INFO - alphas:tensor([0.2216, 0.2012, 0.1915, 0.1918, 0.1939], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,663 - train - INFO - alphas:tensor([0.2222, 0.2014, 0.1906, 0.1915, 0.1943], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,663 - train - INFO - alphas:tensor([0.2231, 0.2009, 0.1915, 0.1922, 0.1923], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,664 - train - INFO - alphas:tensor([0.2239, 0.2033, 0.1894, 0.1907, 0.1927], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,665 - train - INFO - alphas:tensor([0.2199, 0.2007, 0.1911, 0.1916, 0.1967], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,666 - train - INFO - alphas:tensor([0.2224, 0.1994, 0.1916, 0.1926, 0.1940], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,666 - train - INFO - alphas:tensor([0.2179, 0.2006, 0.1920, 0.1915, 0.1980], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:45:54,667 - train - INFO - lasso_alpha:1e-05
2024-01-30 16:45:54,958 - train - INFO - Test: [   0/39]  Time: 0.281 (0.281s) Loss:  0.5415 (0.5415)  Acc@1: 90.2344 (90.2344)  Acc@5: 99.6094 (99.6094)
2024-01-30 16:46:00,857 - train - INFO - Test: [  39/39]  Time: 0.140 (6.180s) Loss:  0.6470 (0.5570)  Acc@1: 75.0000 (88.5600)  Acc@5: 100.0000 (99.4100)
2024-01-30 16:46:01,941 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  1.369736 (1.3697)  Time: 0.893s (0.893s),  286.70/s  (0.893s,  286.70/s)  LR: 5.500e-04  Data: 0.248 (0.248)
2024-01-30 16:46:30,805 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  1.498386 (1.5114)  Time: 0.563s (29.755s),  454.64/s  (0.583s,  438.78/s)  LR: 5.500e-04  Data: 0.007 (0.013)
2024-01-30 16:47:00,193 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  1.669570 (1.4985)  Time: 0.551s (59.141s),  464.52/s  (0.586s,  437.19/s)  LR: 5.500e-04  Data: 0.008 (0.011)
2024-01-30 16:47:27,455 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  1.363983 (1.4875)  Time: 0.533s (86.401s),  480.27/s  (0.572s,  447.40/s)  LR: 5.500e-04  Data: 0.006 (0.010)
2024-01-30 16:47:51,102 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  1.524474 (1.4923)  Time: 0.525s (110.047s),  487.99/s  (0.564s,  453.62/s)  LR: 5.500e-04  Data: 0.000 (0.009)
2024-01-30 16:47:51,104 - train - INFO - alphas:tensor([0.3795, 0.3129, 0.3076], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,105 - train - INFO - alphas:tensor([0.3770, 0.3111, 0.3119], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,106 - train - INFO - alphas:tensor([0.3841, 0.3098, 0.3061], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,108 - train - INFO - alphas:tensor([0.3773, 0.3151, 0.3076], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,109 - train - INFO - alphas:tensor([0.3817, 0.3092, 0.3091], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,110 - train - INFO - alphas:tensor([0.2278, 0.1992, 0.1869, 0.1908, 0.1953], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,111 - train - INFO - alphas:tensor([0.3879, 0.3074, 0.3047], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,111 - train - INFO - alphas:tensor([0.3814, 0.3116, 0.3070], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,112 - train - INFO - alphas:tensor([0.3860, 0.3087, 0.3053], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,113 - train - INFO - alphas:tensor([0.3804, 0.3105, 0.3091], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,114 - train - INFO - alphas:tensor([0.2343, 0.2017, 0.1850, 0.1872, 0.1918], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,115 - train - INFO - alphas:tensor([0.2319, 0.2009, 0.1877, 0.1883, 0.1912], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,116 - train - INFO - alphas:tensor([0.2365, 0.2007, 0.1864, 0.1855, 0.1909], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,117 - train - INFO - alphas:tensor([0.2359, 0.1999, 0.1842, 0.1867, 0.1933], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,118 - train - INFO - alphas:tensor([0.2374, 0.2038, 0.1833, 0.1844, 0.1910], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,119 - train - INFO - alphas:tensor([0.2359, 0.2028, 0.1826, 0.1859, 0.1928], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,120 - train - INFO - alphas:tensor([0.2395, 0.1998, 0.1848, 0.1857, 0.1902], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,121 - train - INFO - alphas:tensor([0.2331, 0.2032, 0.1842, 0.1861, 0.1934], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,122 - train - INFO - alphas:tensor([0.2419, 0.1962, 0.1850, 0.1868, 0.1901], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,122 - train - INFO - alphas:tensor([0.2390, 0.1990, 0.1850, 0.1870, 0.1900], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,123 - train - INFO - alphas:tensor([0.2420, 0.1983, 0.1839, 0.1865, 0.1893], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,124 - train - INFO - alphas:tensor([0.2378, 0.2000, 0.1851, 0.1871, 0.1900], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,125 - train - INFO - alphas:tensor([0.2431, 0.1978, 0.1826, 0.1858, 0.1906], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,126 - train - INFO - alphas:tensor([0.2396, 0.2000, 0.1829, 0.1853, 0.1921], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,126 - train - INFO - alphas:tensor([0.2418, 0.1987, 0.1845, 0.1859, 0.1891], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,127 - train - INFO - alphas:tensor([0.2393, 0.1996, 0.1849, 0.1859, 0.1903], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,128 - train - INFO - alphas:tensor([0.2418, 0.2005, 0.1829, 0.1849, 0.1898], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,129 - train - INFO - alphas:tensor([0.2427, 0.1975, 0.1844, 0.1861, 0.1892], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,130 - train - INFO - alphas:tensor([0.2443, 0.2021, 0.1823, 0.1842, 0.1870], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,130 - train - INFO - alphas:tensor([0.2366, 0.1995, 0.1831, 0.1854, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,131 - train - INFO - alphas:tensor([0.2425, 0.1974, 0.1845, 0.1862, 0.1894], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,132 - train - INFO - alphas:tensor([0.2325, 0.2010, 0.1852, 0.1848, 0.1965], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:47:51,132 - train - INFO - lasso_alpha:1e-05
2024-01-30 16:47:51,471 - train - INFO - Test: [   0/39]  Time: 0.329 (0.329s) Loss:  0.5215 (0.5215)  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:47:57,981 - train - INFO - Test: [  39/39]  Time: 0.134 (6.838s) Loss:  0.4844 (0.5174)  Acc@1: 93.7500 (90.7800)  Acc@5: 100.0000 (99.6100)
2024-01-30 16:47:59,107 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  1.686625 (1.6866)  Time: 0.958s (0.958s),  267.36/s  (0.958s,  267.36/s)  LR: 5.500e-04  Data: 0.279 (0.279)
