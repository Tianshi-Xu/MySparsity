2024-01-30 16:38:34,651 - train - INFO - aa: rand-m9-mstd0.5-inc1
amp: true
apex_amp: false
aq_asym: false
aq_bitw: null
aq_enable: false
aq_mode: lsq
aq_neg: null
aq_pos: null
as_enable: false
as_mode: Identity
as_patterns: '1:1'
aug_splits: 0
batch_size: 256
bn_eps: null
bn_momentum: null
bn_tf: false
channels_last: false
checkpoint_hist: 5
clip_grad: null
clip_mode: norm
color_jitter: 0.4
cooldown_epochs: 10
crop_pct: 1.0
cutmix: 1.0
cutmix_minmax: null
data_dir: /home/xts/code/dataset/cifar10
dataset: torch/cifar10
decay_epochs: 30
decay_rate: 0.1
dist_bn: ''
distributed: false
down_block_type: default
drop: 0.0
drop_block: null
drop_connect: null
drop_path: null
epoch_repeats: 0.0
epochs: 400
eval_metric: top1
experiment: ''
finetune: true
gp: null
hflip: 0.5
img_size: 32
initial_checkpoint: /home/xts/code/neujeans/MySparsity/Ternary-ViT/output/train/20240130-151816-pretrain_cifar_cir_nas_mobilenetv2-32/model_best.pth.tar
input_size: null
interpolation: bicubic
jsd: false
kd_alpha: 4
kd_type: last
lasso_alpha: 1.0e-05
local_rank: 0
log_interval: 50
log_wandb: false
lr: 0.00055
lr_cycle_limit: 1
lr_cycle_mul: 1.0
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
mean:
- 0.4914
- 0.4822
- 0.4465
min_lr: 1.0e-05
mixup: 0.8
mixup_mode: batch
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_switch_prob: 0.5
model: cifar_cir_nas_mobilenetv2
model_ema: false
model_ema_decay: 0.9998
model_ema_force_cpu: false
momentum: 0.9
native_amp: false
no_aug: false
no_bn: false
no_prefetcher: false
no_resume_opt: false
num_classes: 10
opt: adamw
opt_betas: null
opt_eps: null
output: ''
patience_epochs: 10
pin_mem: false
post_res_bn: false
pretrain: false
pretrained: false
qmodules: []
quant_teacher: false
ratio:
- 0.75
- 1.3333333333333333
recount: 1
recovery_interval: 0
remode: pixel
replace_ln_by_bn: false
replace_relu: false
reprob: 0.25
resplit: false
resq_asym: false
resq_bitw: null
resq_enable: false
resq_mode: lsq
resq_modules: []
resq_neg: null
resq_pos: null
resume: ''
save_images: false
scale:
- 0.8
- 1.0
sched: cosine
seed: 3407
smoothing: 0.1
split_bn: false
start_epoch: null
std:
- 0.247
- 0.2435
- 0.2616
sync_bn: false
teacher: pretrain_cifar_cir_nas_mobilenetv2
teacher_checkpoint: /home/xts/code/neujeans/MySparsity/Ternary-ViT/output/train/20240130-151816-pretrain_cifar_cir_nas_mobilenetv2-32/model_best.pth.tar
torchscript: false
train_interpolation: random
train_split: train
tta: 0
use_bn: true
use_distill_head: false
use_dual_skip: false
use_kd: true
use_layer_scale: false
use_multi_epochs_loader: false
use_relu: false
use_skip: false
use_token_kd: false
val_split: validation
validation_batch_size_multiplier: 1
vflip: 0.0
warmup_epochs: 0
warmup_lr: 1.0e-05
weight_decay: 0.06
workers: 32
wq_asym: false
wq_bitw: null
wq_enable: false
wq_mode: LSQ
wq_neg: null
wq_per_channel: false
wq_pos: null
ws_enable: false
ws_mode: Identity
ws_patterns: '1:1'

2024-01-30 16:38:34,651 - train - INFO - Training with a single process on 1 GPUs.
2024-01-30 16:38:34,803 - train - INFO - Model CirNasMobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): CirNasInvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): CirNasInvertedResidual(
      (conv): Sequential(
        (0): LearnableCir()
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): LearnableCir()
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=10, bias=True)
  )
)
pretrain: False
fine-tune: False
2024-01-30 16:38:36,013 - train - INFO - Model cifar_cir_nas_mobilenetv2 created, param count:2236824
2024-01-30 16:38:36,025 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-01-30 16:38:36,025 - train - INFO - Scheduled epochs: 410
2024-01-30 16:38:37,467 - train - INFO - Trainer transform: Compose(
    RandomResizedCropAndInterpolation(size=(32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear bicubic)
    RandomHorizontalFlip(p=0.5)
    RandAugment(n=2, ops=
	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
    <timm.data.transforms.ToNumpy object at 0x7fa0a5bdebb0>
)
2024-01-30 16:38:37,467 - train - INFO - Validate transform: Compose(
    Resize(size=32, interpolation=bicubic, max_size=None, antialias=warn)
    CenterCrop(size=(32, 32))
    <timm.data.transforms.ToNumpy object at 0x7fa0a5bde700>
)
2024-01-30 16:38:37,468 - train - INFO - Train loss CrossEntropyLoss()
2024-01-30 16:38:37,468 - train - INFO - Verifying teacher model
2024-01-30 16:38:40,499 - train - INFO - Test: [   0/39]  Time: 3.030 (3.030s) Loss:  0.3721 (0.3721)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:38:41,159 - train - INFO - Test: [  39/39]  Time: 0.389 (3.690s) Loss:  0.4368 (0.3827)  Acc@1: 93.7500 (94.7500)  Acc@5: 100.0000 (99.8400)
2024-01-30 16:38:41,159 - train - INFO - Verifying initial model
2024-01-30 16:38:41,499 - train - INFO - Test: [   0/39]  Time: 0.338 (0.338s) Loss:  3.0488 (3.0488)  Acc@1:  8.9844 ( 8.9844)  Acc@5: 48.8281 (48.8281)
2024-01-30 16:38:48,319 - train - INFO - Test: [  39/39]  Time: 0.125 (7.158s) Loss:  2.7207 (2.9991)  Acc@1: 18.7500 (10.0000)  Acc@5: 50.0000 (50.0000)
2024-01-30 16:38:53,195 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  2.130087 (2.1301)  Time: 4.873s (4.873s),   52.54/s  (4.873s,   52.54/s)  LR: 5.500e-04  Data: 2.324 (2.324)
2024-01-30 16:39:20,173 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  1.751960 (1.7611)  Time: 0.562s (31.848s),  455.31/s  (0.624s,  409.95/s)  LR: 5.500e-04  Data: 0.019 (0.054)
2024-01-30 16:39:48,832 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  1.554292 (1.6808)  Time: 0.609s (60.504s),  420.26/s  (0.599s,  427.34/s)  LR: 5.500e-04  Data: 0.009 (0.031)
2024-01-30 16:40:18,745 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  1.739288 (1.6504)  Time: 0.555s (90.416s),  460.99/s  (0.599s,  427.54/s)  LR: 5.500e-04  Data: 0.007 (0.024)
2024-01-30 16:40:43,083 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  1.384376 (1.6236)  Time: 0.549s (114.752s),  466.39/s  (0.588s,  435.03/s)  LR: 5.500e-04  Data: 0.000 (0.020)
2024-01-30 16:40:43,100 - train - INFO - alphas:tensor([0.3583, 0.3217, 0.3199], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,101 - train - INFO - alphas:tensor([0.3601, 0.3194, 0.3205], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,102 - train - INFO - alphas:tensor([0.3634, 0.3203, 0.3163], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,103 - train - INFO - alphas:tensor([0.3579, 0.3227, 0.3194], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,104 - train - INFO - alphas:tensor([0.3609, 0.3196, 0.3195], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,105 - train - INFO - alphas:tensor([0.2161, 0.2006, 0.1926, 0.1943, 0.1965], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,106 - train - INFO - alphas:tensor([0.3611, 0.3205, 0.3185], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,107 - train - INFO - alphas:tensor([0.3600, 0.3217, 0.3183], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,108 - train - INFO - alphas:tensor([0.3606, 0.3191, 0.3203], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,109 - train - INFO - alphas:tensor([0.3592, 0.3220, 0.3188], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,110 - train - INFO - alphas:tensor([0.2193, 0.2029, 0.1911, 0.1923, 0.1945], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,111 - train - INFO - alphas:tensor([0.2173, 0.2031, 0.1924, 0.1925, 0.1947], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,112 - train - INFO - alphas:tensor([0.2195, 0.2026, 0.1923, 0.1916, 0.1939], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,113 - train - INFO - alphas:tensor([0.2205, 0.2011, 0.1904, 0.1916, 0.1965], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,114 - train - INFO - alphas:tensor([0.2206, 0.2025, 0.1902, 0.1918, 0.1949], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,115 - train - INFO - alphas:tensor([0.2191, 0.2030, 0.1903, 0.1925, 0.1952], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,116 - train - INFO - alphas:tensor([0.2209, 0.2010, 0.1913, 0.1916, 0.1952], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,116 - train - INFO - alphas:tensor([0.2179, 0.2018, 0.1907, 0.1922, 0.1973], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,117 - train - INFO - alphas:tensor([0.2226, 0.1996, 0.1915, 0.1921, 0.1942], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,118 - train - INFO - alphas:tensor([0.2216, 0.2009, 0.1909, 0.1922, 0.1944], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,119 - train - INFO - alphas:tensor([0.2229, 0.2005, 0.1909, 0.1922, 0.1935], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,120 - train - INFO - alphas:tensor([0.2200, 0.2003, 0.1922, 0.1935, 0.1940], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,120 - train - INFO - alphas:tensor([0.2234, 0.1998, 0.1900, 0.1915, 0.1953], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,121 - train - INFO - alphas:tensor([0.2218, 0.2012, 0.1903, 0.1913, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,122 - train - INFO - alphas:tensor([0.2223, 0.2016, 0.1907, 0.1916, 0.1937], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,123 - train - INFO - alphas:tensor([0.2219, 0.2016, 0.1914, 0.1918, 0.1934], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,124 - train - INFO - alphas:tensor([0.2225, 0.2022, 0.1904, 0.1912, 0.1937], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,124 - train - INFO - alphas:tensor([0.2232, 0.2013, 0.1908, 0.1919, 0.1927], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,125 - train - INFO - alphas:tensor([0.2242, 0.2031, 0.1891, 0.1911, 0.1924], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,126 - train - INFO - alphas:tensor([0.2203, 0.2013, 0.1911, 0.1908, 0.1965], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,127 - train - INFO - alphas:tensor([0.2224, 0.1993, 0.1917, 0.1926, 0.1940], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,128 - train - INFO - alphas:tensor([0.2175, 0.2007, 0.1919, 0.1915, 0.1984], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:40:43,128 - train - INFO - lasso_alpha:1e-05
2024-01-30 16:40:43,436 - train - INFO - Test: [   0/39]  Time: 0.294 (0.294s) Loss:  0.5327 (0.5327)  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:40:48,767 - train - INFO - Test: [  39/39]  Time: 0.130 (5.625s) Loss:  0.7188 (0.5534)  Acc@1: 75.0000 (88.3800)  Acc@5: 100.0000 (99.4400)
2024-01-30 16:40:49,730 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  1.385624 (1.3856)  Time: 0.869s (0.869s),  294.51/s  (0.869s,  294.51/s)  LR: 5.500e-04  Data: 0.257 (0.257)
2024-01-30 16:41:16,931 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  1.497941 (1.5114)  Time: 0.542s (28.069s),  471.96/s  (0.550s,  465.14/s)  LR: 5.500e-04  Data: 0.006 (0.012)
2024-01-30 16:41:44,379 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  1.681994 (1.4994)  Time: 0.507s (55.515s),  504.52/s  (0.550s,  465.75/s)  LR: 5.500e-04  Data: 0.008 (0.010)
2024-01-30 16:42:14,720 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  1.366794 (1.4878)  Time: 0.617s (85.855s),  414.92/s  (0.569s,  450.25/s)  LR: 5.500e-04  Data: 0.009 (0.009)
2024-01-30 16:42:42,126 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  1.527951 (1.4920)  Time: 0.600s (113.260s),  426.38/s  (0.581s,  440.76/s)  LR: 5.500e-04  Data: 0.000 (0.009)
2024-01-30 16:42:42,129 - train - INFO - alphas:tensor([0.3780, 0.3125, 0.3095], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,130 - train - INFO - alphas:tensor([0.3772, 0.3110, 0.3119], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,131 - train - INFO - alphas:tensor([0.3840, 0.3098, 0.3062], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,133 - train - INFO - alphas:tensor([0.3752, 0.3156, 0.3092], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,134 - train - INFO - alphas:tensor([0.3817, 0.3095, 0.3089], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,135 - train - INFO - alphas:tensor([0.2275, 0.1999, 0.1877, 0.1911, 0.1937], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,136 - train - INFO - alphas:tensor([0.3857, 0.3084, 0.3059], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,137 - train - INFO - alphas:tensor([0.3793, 0.3124, 0.3083], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,138 - train - INFO - alphas:tensor([0.3848, 0.3074, 0.3079], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,139 - train - INFO - alphas:tensor([0.3814, 0.3100, 0.3086], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,140 - train - INFO - alphas:tensor([0.2335, 0.2022, 0.1850, 0.1877, 0.1915], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,140 - train - INFO - alphas:tensor([0.2318, 0.2009, 0.1869, 0.1881, 0.1923], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,141 - train - INFO - alphas:tensor([0.2372, 0.2005, 0.1858, 0.1859, 0.1906], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,142 - train - INFO - alphas:tensor([0.2358, 0.2010, 0.1837, 0.1860, 0.1936], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,143 - train - INFO - alphas:tensor([0.2373, 0.2030, 0.1837, 0.1853, 0.1907], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,145 - train - INFO - alphas:tensor([0.2355, 0.2026, 0.1834, 0.1861, 0.1925], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,146 - train - INFO - alphas:tensor([0.2400, 0.1986, 0.1845, 0.1860, 0.1909], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,146 - train - INFO - alphas:tensor([0.2341, 0.2015, 0.1835, 0.1866, 0.1943], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,147 - train - INFO - alphas:tensor([0.2418, 0.1958, 0.1851, 0.1869, 0.1905], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,148 - train - INFO - alphas:tensor([0.2394, 0.2000, 0.1846, 0.1864, 0.1897], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,149 - train - INFO - alphas:tensor([0.2422, 0.1989, 0.1837, 0.1856, 0.1895], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,150 - train - INFO - alphas:tensor([0.2379, 0.2003, 0.1855, 0.1878, 0.1887], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,150 - train - INFO - alphas:tensor([0.2439, 0.1976, 0.1828, 0.1852, 0.1905], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,151 - train - INFO - alphas:tensor([0.2403, 0.1995, 0.1830, 0.1850, 0.1921], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,152 - train - INFO - alphas:tensor([0.2416, 0.1990, 0.1839, 0.1861, 0.1894], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,153 - train - INFO - alphas:tensor([0.2402, 0.2002, 0.1849, 0.1854, 0.1893], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,154 - train - INFO - alphas:tensor([0.2419, 0.2002, 0.1833, 0.1851, 0.1895], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,154 - train - INFO - alphas:tensor([0.2432, 0.1984, 0.1830, 0.1856, 0.1898], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,155 - train - INFO - alphas:tensor([0.2449, 0.2014, 0.1818, 0.1846, 0.1872], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,156 - train - INFO - alphas:tensor([0.2372, 0.1996, 0.1837, 0.1850, 0.1946], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,157 - train - INFO - alphas:tensor([0.2432, 0.1973, 0.1843, 0.1860, 0.1893], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,158 - train - INFO - alphas:tensor([0.2327, 0.2009, 0.1849, 0.1849, 0.1966], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:42:42,158 - train - INFO - lasso_alpha:1e-05
2024-01-30 16:42:42,465 - train - INFO - Test: [   0/39]  Time: 0.298 (0.298s) Loss:  0.5249 (0.5249)  Acc@1: 89.8438 (89.8438)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:42:47,759 - train - INFO - Test: [  39/39]  Time: 0.133 (5.592s) Loss:  0.5571 (0.5192)  Acc@1: 87.5000 (90.4400)  Acc@5: 100.0000 (99.5600)
2024-01-30 16:42:48,735 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  1.677416 (1.6774)  Time: 0.884s (0.884s),  289.52/s  (0.884s,  289.52/s)  LR: 5.500e-04  Data: 0.213 (0.213)
2024-01-30 16:43:17,658 - train - INFO - Train: 2 [  50/195 ( 26%)]  Loss:  1.562420 (1.4917)  Time: 0.500s (29.807s),  511.78/s  (0.584s,  438.02/s)  LR: 5.500e-04  Data: 0.006 (0.012)
2024-01-30 16:43:44,037 - train - INFO - Train: 2 [ 100/195 ( 52%)]  Loss:  1.345221 (1.4876)  Time: 0.541s (56.184s),  472.97/s  (0.556s,  460.20/s)  LR: 5.500e-04  Data: 0.007 (0.010)
2024-01-30 16:44:13,443 - train - INFO - Train: 2 [ 150/195 ( 77%)]  Loss:  1.625594 (1.4844)  Time: 0.560s (85.589s),  457.05/s  (0.567s,  451.65/s)  LR: 5.500e-04  Data: 0.008 (0.009)
2024-01-30 16:44:38,518 - train - INFO - Train: 2 [ 194/195 (100%)]  Loss:  1.513376 (1.4840)  Time: 0.722s (110.663s),  354.72/s  (0.568s,  451.10/s)  LR: 5.500e-04  Data: 0.000 (0.009)
2024-01-30 16:44:38,521 - train - INFO - alphas:tensor([0.3949, 0.3039, 0.3012], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,522 - train - INFO - alphas:tensor([0.3934, 0.3028, 0.3038], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,523 - train - INFO - alphas:tensor([0.4036, 0.2987, 0.2976], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,524 - train - INFO - alphas:tensor([0.3891, 0.3089, 0.3020], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,525 - train - INFO - alphas:tensor([0.4000, 0.3006, 0.2993], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,526 - train - INFO - alphas:tensor([0.2383, 0.1974, 0.1838, 0.1880, 0.1925], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,527 - train - INFO - alphas:tensor([0.4050, 0.2988, 0.2963], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,528 - train - INFO - alphas:tensor([0.3974, 0.3039, 0.2987], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,529 - train - INFO - alphas:tensor([0.4041, 0.2981, 0.2978], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,530 - train - INFO - alphas:tensor([0.4011, 0.3002, 0.2987], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,530 - train - INFO - alphas:tensor([0.2473, 0.1998, 0.1799, 0.1835, 0.1895], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,531 - train - INFO - alphas:tensor([0.2443, 0.1989, 0.1821, 0.1850, 0.1897], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,532 - train - INFO - alphas:tensor([0.2543, 0.1978, 0.1802, 0.1802, 0.1874], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,533 - train - INFO - alphas:tensor([0.2521, 0.1971, 0.1774, 0.1811, 0.1923], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,534 - train - INFO - alphas:tensor([0.2546, 0.2002, 0.1773, 0.1801, 0.1878], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,535 - train - INFO - alphas:tensor([0.2512, 0.2011, 0.1771, 0.1813, 0.1895], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,536 - train - INFO - alphas:tensor([0.2575, 0.1964, 0.1775, 0.1813, 0.1874], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,537 - train - INFO - alphas:tensor([0.2490, 0.1983, 0.1776, 0.1818, 0.1933], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,538 - train - INFO - alphas:tensor([0.2599, 0.1920, 0.1794, 0.1818, 0.1869], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,539 - train - INFO - alphas:tensor([0.2566, 0.1961, 0.1790, 0.1823, 0.1860], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,539 - train - INFO - alphas:tensor([0.2603, 0.1957, 0.1777, 0.1806, 0.1858], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,540 - train - INFO - alphas:tensor([0.2541, 0.1974, 0.1793, 0.1829, 0.1863], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,541 - train - INFO - alphas:tensor([0.2631, 0.1941, 0.1767, 0.1796, 0.1866], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,542 - train - INFO - alphas:tensor([0.2566, 0.1972, 0.1761, 0.1798, 0.1902], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,543 - train - INFO - alphas:tensor([0.2594, 0.1961, 0.1779, 0.1811, 0.1856], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,543 - train - INFO - alphas:tensor([0.2571, 0.1974, 0.1793, 0.1804, 0.1859], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,544 - train - INFO - alphas:tensor([0.2614, 0.1961, 0.1770, 0.1797, 0.1859], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,545 - train - INFO - alphas:tensor([0.2615, 0.1948, 0.1766, 0.1799, 0.1872], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,546 - train - INFO - alphas:tensor([0.2651, 0.1978, 0.1754, 0.1789, 0.1827], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,547 - train - INFO - alphas:tensor([0.2526, 0.1958, 0.1768, 0.1804, 0.1944], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,547 - train - INFO - alphas:tensor([0.2628, 0.1931, 0.1785, 0.1802, 0.1854], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,548 - train - INFO - alphas:tensor([0.2478, 0.1995, 0.1790, 0.1791, 0.1946], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:44:38,548 - train - INFO - lasso_alpha:1e-05
2024-01-30 16:44:38,881 - train - INFO - Test: [   0/39]  Time: 0.322 (0.322s) Loss:  0.4932 (0.4932)  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:44:45,149 - train - INFO - Test: [  39/39]  Time: 0.172 (6.590s) Loss:  0.5508 (0.5071)  Acc@1: 87.5000 (90.5900)  Acc@5: 100.0000 (99.6300)
2024-01-30 16:44:46,130 - train - INFO - Train: 3 [   0/195 (  0%)]  Loss:  1.254064 (1.2541)  Time: 0.868s (0.868s),  294.79/s  (0.868s,  294.79/s)  LR: 5.499e-04  Data: 0.290 (0.290)
2024-01-30 16:45:13,145 - train - INFO - Train: 3 [  50/195 ( 26%)]  Loss:  1.579833 (1.4667)  Time: 0.537s (27.882s),  476.55/s  (0.547s,  468.26/s)  LR: 5.499e-04  Data: 0.008 (0.013)
2024-01-30 16:45:39,977 - train - INFO - Train: 3 [ 100/195 ( 52%)]  Loss:  1.505977 (1.4624)  Time: 0.513s (54.712s),  498.74/s  (0.542s,  472.58/s)  LR: 5.499e-04  Data: 0.007 (0.010)
2024-01-30 16:46:07,921 - train - INFO - Train: 3 [ 150/195 ( 77%)]  Loss:  1.323822 (1.4597)  Time: 0.522s (82.655s),  490.74/s  (0.547s,  467.68/s)  LR: 5.499e-04  Data: 0.007 (0.009)
2024-01-30 16:46:32,367 - train - INFO - Train: 3 [ 194/195 (100%)]  Loss:  1.509628 (1.4576)  Time: 0.542s (107.098s),  472.27/s  (0.549s,  466.12/s)  LR: 5.499e-04  Data: 0.000 (0.009)
2024-01-30 16:46:32,372 - train - INFO - alphas:tensor([0.4099, 0.2976, 0.2925], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,373 - train - INFO - alphas:tensor([0.4069, 0.2949, 0.2981], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,374 - train - INFO - alphas:tensor([0.4188, 0.2918, 0.2895], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,375 - train - INFO - alphas:tensor([0.4032, 0.3020, 0.2948], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,376 - train - INFO - alphas:tensor([0.4143, 0.2938, 0.2919], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,377 - train - INFO - alphas:tensor([0.2475, 0.1943, 0.1808, 0.1858, 0.1917], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,378 - train - INFO - alphas:tensor([0.4233, 0.2898, 0.2869], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,379 - train - INFO - alphas:tensor([0.4147, 0.2946, 0.2906], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,380 - train - INFO - alphas:tensor([0.4223, 0.2887, 0.2890], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,381 - train - INFO - alphas:tensor([0.4165, 0.2919, 0.2916], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,382 - train - INFO - alphas:tensor([0.2616, 0.1973, 0.1749, 0.1792, 0.1870], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,383 - train - INFO - alphas:tensor([0.2568, 0.1953, 0.1776, 0.1822, 0.1881], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,384 - train - INFO - alphas:tensor([0.2710, 0.1945, 0.1747, 0.1758, 0.1840], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,385 - train - INFO - alphas:tensor([0.2672, 0.1940, 0.1728, 0.1768, 0.1892], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,386 - train - INFO - alphas:tensor([0.2708, 0.1967, 0.1718, 0.1756, 0.1851], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,387 - train - INFO - alphas:tensor([0.2664, 0.1967, 0.1711, 0.1766, 0.1892], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,387 - train - INFO - alphas:tensor([0.2739, 0.1940, 0.1715, 0.1765, 0.1841], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,388 - train - INFO - alphas:tensor([0.2622, 0.1939, 0.1719, 0.1784, 0.1936], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,389 - train - INFO - alphas:tensor([0.2768, 0.1870, 0.1747, 0.1777, 0.1838], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,390 - train - INFO - alphas:tensor([0.2729, 0.1923, 0.1739, 0.1778, 0.1830], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,391 - train - INFO - alphas:tensor([0.2781, 0.1924, 0.1717, 0.1756, 0.1822], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,392 - train - INFO - alphas:tensor([0.2717, 0.1937, 0.1732, 0.1785, 0.1829], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,393 - train - INFO - alphas:tensor([0.2804, 0.1911, 0.1710, 0.1745, 0.1830], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,393 - train - INFO - alphas:tensor([0.2717, 0.1945, 0.1709, 0.1750, 0.1880], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,394 - train - INFO - alphas:tensor([0.2777, 0.1911, 0.1723, 0.1768, 0.1821], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,395 - train - INFO - alphas:tensor([0.2737, 0.1939, 0.1739, 0.1756, 0.1829], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,396 - train - INFO - alphas:tensor([0.2789, 0.1922, 0.1715, 0.1751, 0.1823], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,397 - train - INFO - alphas:tensor([0.2783, 0.1897, 0.1705, 0.1754, 0.1860], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,398 - train - INFO - alphas:tensor([0.2859, 0.1918, 0.1692, 0.1741, 0.1791], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,398 - train - INFO - alphas:tensor([0.2674, 0.1919, 0.1708, 0.1755, 0.1944], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,399 - train - INFO - alphas:tensor([0.2810, 0.1897, 0.1730, 0.1749, 0.1814], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,401 - train - INFO - alphas:tensor([0.2602, 0.1978, 0.1738, 0.1745, 0.1937], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-01-30 16:46:32,401 - train - INFO - lasso_alpha:1e-05
2024-01-30 16:46:32,765 - train - INFO - Test: [   0/39]  Time: 0.350 (0.350s) Loss:  0.4717 (0.4717)  Acc@1: 91.4062 (91.4062)  Acc@5: 100.0000 (100.0000)
2024-01-30 16:46:39,031 - train - INFO - Test: [  39/39]  Time: 0.129 (6.616s) Loss:  0.4951 (0.4791)  Acc@1: 93.7500 (92.0900)  Acc@5: 100.0000 (99.7300)
2024-01-30 16:46:40,167 - train - INFO - Train: 4 [   0/195 (  0%)]  Loss:  1.447334 (1.4473)  Time: 1.020s (1.020s),  251.10/s  (1.020s,  251.10/s)  LR: 5.499e-04  Data: 0.321 (0.321)
2024-01-30 16:47:06,856 - train - INFO - Train: 4 [  50/195 ( 26%)]  Loss:  1.431369 (1.4697)  Time: 0.552s (27.707s),  464.06/s  (0.543s,  471.22/s)  LR: 5.499e-04  Data: 0.004 (0.013)
2024-01-30 16:47:34,735 - train - INFO - Train: 4 [ 100/195 ( 52%)]  Loss:  1.385126 (1.4464)  Time: 0.534s (55.583s),  479.30/s  (0.550s,  465.18/s)  LR: 5.499e-04  Data: 0.008 (0.010)
